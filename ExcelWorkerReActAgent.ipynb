{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1241ff06-5e18-4051-8d58-5109da79f506",
   "metadata": {},
   "source": [
    "<h1>Excel Worker - LangCahin ReaAct Agent</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847082fd-86f2-4118-b82c-1ff6902f987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langgraph pandas python-dotenv duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc549730-aea1-45f2-99d8-9bfc4f22991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "import json\n",
    "from typing import TypedDict, Optional, List, Dict, Union\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.tools import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from IPython.display import display\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ce833-402f-4583-9aba-c8ef524091ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State management\n",
    "def create_agent_state(file_name: str, user_query: str) -> dict:\n",
    "    return {\n",
    "        \"file_name\": file_name,\n",
    "        \"user_query\": user_query,\n",
    "        \"messages\": [],\n",
    "        \"preview_data\": None,\n",
    "        \"last_query\": None,\n",
    "        \"query_result\": None,\n",
    "        \"error\": None,\n",
    "    }\n",
    "\n",
    "def print_state(state: dict, title: str = \"Current State\"):\n",
    "    \"\"\"Print current state of the agent\"\"\"\n",
    "    print(f\"\\n{'='*20} {title} {'='*20}\")\n",
    "    print(f\"File: {state['file_name']}\")\n",
    "    print(f\"User Query: {state['user_query']}\")\n",
    "    \n",
    "    if state['error']:\n",
    "        print(f\"\\nüî¥ Error: {state['error']}\")\n",
    "        \n",
    "    if state['last_query']:\n",
    "        print(\"\\nüìù Last Query:\")\n",
    "        print(state['last_query'])\n",
    "        \n",
    "    if state['query_result']:\n",
    "        print(\"\\n‚úÖ Query Result:\")\n",
    "        if isinstance(state['query_result'], dict) and 'rows' in state['query_result']:\n",
    "            df = pd.DataFrame(state['query_result']['rows'])\n",
    "            display(df)\n",
    "        else:\n",
    "            print(json.dumps(state['query_result'], indent=2))\n",
    "    print(f\"\\n{'='*50}\\n\")\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Robust preprocessing of DataFrame to handle empty values.\"\"\"\n",
    "    # Convert empty strings and whitespace to None\n",
    "    df = df.replace(r'^\\s*$', None, regex=True)\n",
    "    # Convert NaN strings to None\n",
    "    df = df.replace(['nan', 'NaN', 'null'], None)\n",
    "    # Convert pandas NaN to None\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    return df\n",
    "\n",
    "# Tool definitions with single string input\n",
    "@tool\n",
    "def preview_excel_structure(input_str: str) -> str:\n",
    "    \"\"\"Use this first to examine the Excel file structure and data types. \n",
    "    The input should be a JSON string with format: {\"file_name\": \"your_file.xlsx\"}\"\"\"\n",
    "    try:\n",
    "        data = json.loads(input_str)\n",
    "        file_name = data.get(\"file_name\")\n",
    "        if not file_name:\n",
    "            return json.dumps({\"error\": \"File name must be provided\"})\n",
    "            \n",
    "        df = pd.read_excel(file_name)\n",
    "        df = preprocess_dataframe(df)  # Apply preprocessing\n",
    "        df_sample = df.head(3).astype(str)\n",
    "        print(\"‚úÖ Preview successful\")\n",
    "        display(df_sample)\n",
    "        return json.dumps({\n",
    "            \"result\": {\n",
    "                \"columns\": df.columns.tolist(),\n",
    "                \"dtypes\": df.dtypes.astype(str).to_dict(),\n",
    "                \"sample_rows\": df_sample.to_dict(orient=\"records\")\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "        \n",
    "@tool\n",
    "def complex_duckdb_query(input_str: str) -> str:\n",
    "    \"\"\"Use this for complex SQL operations (GROUP BY, aggregations, etc.).\n",
    "    The input should be a JSON string with format: \n",
    "    {\"file_name\": \"your_file.xlsx\", \"query\": \"your SQL query\"}\n",
    "    Note: Use 'data' as the table name in your SQL queries.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(input_str)\n",
    "        file_name = data.get(\"file_name\")\n",
    "        query = data.get(\"query\")\n",
    "        if not file_name or not query:\n",
    "            return json.dumps({\"error\": \"Both file_name and query must be provided\"})\n",
    "\n",
    "        print(\"\\nüîç Executing DuckDB query:\")\n",
    "        print(query)\n",
    "        \n",
    "        # Read and preprocess the Excel file to handle empty values\n",
    "        df = pd.read_excel(file_name)\n",
    "        df = preprocess_dataframe(df)  # Clean the data upfront\n",
    "        \n",
    "        with duckdb.connect() as con:\n",
    "            # Register dataframe with a consistent table name 'data'\n",
    "            con.register(\"data\", df)\n",
    "            result = con.execute(query).fetchdf()\n",
    "            print(\"‚úÖ Query successful\")\n",
    "            display(result)\n",
    "            \n",
    "            # Handle different types of results robustly\n",
    "            if result is None:\n",
    "                return json.dumps({\"result\": None})\n",
    "                \n",
    "            if isinstance(result, pd.DataFrame):\n",
    "                # Handle DataFrame results\n",
    "                try:\n",
    "                    # Replace various null/nan types with None\n",
    "                    df_processed = result.copy()\n",
    "                    \n",
    "                    # Handle infinite values\n",
    "                    df_processed = df_processed.replace([float('inf'), -float('inf')], None)\n",
    "                    \n",
    "                    # Replace numpy NaN, pandas NA, and other null types with None\n",
    "                    df_processed = df_processed.where(pd.notna(df_processed), None)\n",
    "                    \n",
    "                    # Ensure all values are JSON serializable\n",
    "                    for column in df_processed.columns:\n",
    "                        if df_processed[column].dtype == 'object':\n",
    "                            df_processed[column] = df_processed[column].apply(\n",
    "                                lambda x: str(x) if x is not None else None\n",
    "                            )\n",
    "                    \n",
    "                    return json.dumps({\n",
    "                        \"result\": {\n",
    "                            \"columns\": df_processed.columns.tolist(),\n",
    "                            \"rows\": df_processed.to_dict(orient=\"records\")\n",
    "                        }\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    return json.dumps({\n",
    "                        \"error\": f\"Error processing DataFrame: {str(e)}\",\n",
    "                        \"result\": result.to_string()  # Fallback to string representation\n",
    "                    })\n",
    "            else:\n",
    "                # Handle scalar or other types of results\n",
    "                try:\n",
    "                    return json.dumps({\"result\": result})\n",
    "                except TypeError:\n",
    "                    return json.dumps({\n",
    "                        \"result\": str(result)  # Fallback to string representation\n",
    "                    })\n",
    "            \n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "@tool\n",
    "def simple_dataframe_query(input_str: str) -> str:\n",
    "    \"\"\"Use this for simple Pandas operations on the dataframe.\n",
    "    The input should be a JSON string with format:\n",
    "    {\"file_name\": \"your_file.xlsx\", \"query\": \"your pandas expression\"}\"\"\"\n",
    "    try:\n",
    "        data = json.loads(input_str)\n",
    "        file_name = data.get(\"file_name\")\n",
    "        query = data.get(\"query\")\n",
    "        if not file_name or not query:\n",
    "            return json.dumps({\"error\": \"Both file_name and query must be provided\"})\n",
    "\n",
    "        print(\"\\nüîç Executing Pandas query:\")\n",
    "        print(query)\n",
    "        \n",
    "        df = pd.read_excel(file_name)\n",
    "        df = preprocess_dataframe(df)  # Apply preprocessing\n",
    "        result = eval(query, {\"df\": df, \"pd\": pd})\n",
    "        \n",
    "        if isinstance(result, pd.DataFrame):\n",
    "            output = {\n",
    "                \"result\": {\n",
    "                    \"columns\": result.columns.tolist(),\n",
    "                    \"rows\": result.fillna(None).to_dict(orient=\"records\")\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            output = {\"result\": str(result)}\n",
    "        \n",
    "        print(\"‚úÖ Query successful\")\n",
    "        if isinstance(result, pd.DataFrame):\n",
    "            display(result)\n",
    "        else:\n",
    "            print(result)\n",
    "            \n",
    "        return json.dumps(output)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "\n",
    "# Tools list\n",
    "tools = [preview_excel_structure, complex_duckdb_query, simple_dataframe_query]\n",
    "\n",
    "# System prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are an Excel analysis expert that helps users analyze Excel files using SQL and Pandas.\n",
    "\n",
    "IMPORTANT WORKFLOW:\n",
    "1. ALWAYS start by using preview_excel_structure to understand the data structure\n",
    "2. Based on the preview, choose the appropriate tool:\n",
    "   - For grouping, aggregations, or complex calculations: use complex_duckdb_query\n",
    "   - For simple row operations or basic calculations: use simple_dataframe_query\n",
    "\n",
    "For DuckDB SQL queries, ALWAYS:\n",
    "1. Handle empty cells in calculations:\n",
    "   - For sums and averages, count only non-null values\n",
    "   - When calculating averages across multiple columns:\n",
    "     * Sum only non-null values from each column\n",
    "     * Divide by the count of non-null values\n",
    "   * For complex calculations, use WITH clauses to split steps, example: WITH individual_calcs AS (SELECT id, CASE WHEN \"Value\" IS NOT NULL THEN \"Value\" END as clean_value FROM data), group_calcs AS (SELECT group_col, SUM(clean_value) / NULLIF(COUNT(clean_value), 0) as avg_value FROM individual_calcs GROUP BY group_col) SELECT * FROM group_calcs\n",
    "\n",
    "2. Cast strings to proper types:\n",
    "   CAST(column AS DOUBLE)\n",
    "\n",
    "3. For numeric ranges use:\n",
    "   CASE WHEN col >= 0 AND col < 5 THEN '0-5' WHEN col >= 5 AND col < 10 THEN '5-10' END as range\n",
    "\n",
    "4. Include all non-aggregated columns in GROUP BY\n",
    "\n",
    "5. Use double quotes for columns with spaces: \"Column Name\"\n",
    "\n",
    "6. For complex aggregations:\n",
    "   - First calculate individual metrics\n",
    "   - Then group and aggregate\n",
    "   - Finally format output\n",
    "   Example query: WITH symbol_metrics AS (SELECT \"Symbol\", (COALESCE(\"Month1\", 0) + COALESCE(\"Month2\", 0)) / NULLIF(CAST(\"Month1\" IS NOT NULL AS INTEGER) + CAST(\"Month2\" IS NOT NULL AS INTEGER), 0) as avg_performance FROM data), group_metrics AS (SELECT group_col, AVG(avg_performance) as group_performance, STRING_AGG(Symbol, ', ') as symbols_in_group FROM symbol_metrics GROUP BY group_col) SELECT * FROM group_metrics\n",
    "\n",
    "For Pandas queries:\n",
    "1. Reference the dataframe as 'df'\n",
    "2. For ranges use: pd.cut(df['column'], bins=[0,5,10], labels=['0-5','5-10'])\n",
    "3. For aggregations use: df.groupby().agg()\n",
    "\n",
    "IMPORTANT PATTERNS FOR COMPLEX ANALYSIS:\n",
    "1. Calculate individual metrics first:\n",
    "   - Handle NULL values with COALESCE\n",
    "   - Count valid values using CAST(IS NOT NULL AS INTEGER)\n",
    "2. Group results:\n",
    "   - Use proper group by\n",
    "   - Include supporting information\n",
    "3. Format final output:\n",
    "   - Order results if needed\n",
    "   - Include all requested information\n",
    "\n",
    "IMPORTANT: All tool inputs must be properly formatted JSON strings. Examples:\n",
    "\n",
    "1. Preview Excel:\n",
    "{\"file_name\": \"example.xlsx\"}\n",
    "\n",
    "2. DuckDB Query with proper empty value handling:\n",
    "{\"file_name\": \"example.xlsx\", \"query\": \"WITH metrics AS (SELECT CASE WHEN price >= 0 AND price < 5 THEN '0-5' ELSE '5+' END as range, value FROM data WHERE value IS NOT NULL) SELECT range, AVG(value) as avg_val FROM metrics GROUP BY range\"}\n",
    "\n",
    "3. Pandas Query:\n",
    "{\"file_name\": \"example.xlsx\", \"query\": \"df.groupby(pd.cut(df['col'], bins=[0,5,10])).agg(['mean', 'count'])\"}\n",
    "\n",
    "Remember:\n",
    "1. Tool inputs must be valid JSON strings\n",
    "2. Preview data first\n",
    "3. Always exclude empty/null values from calculations\n",
    "4. Use proper column references\n",
    "5. Queries must be on a single line (no line breaks in JSON strings)\n",
    "6. For complex analyses:\n",
    "   - Break calculations into steps using WITH clauses\n",
    "   - Calculate individual metrics before grouping\n",
    "   - Include supporting information (counts, lists) in output\"\"\"\n",
    "\n",
    "\n",
    "# Agent setup remains the same\n",
    "def setup_agent(debug=True):\n",
    "    \"\"\"Initialize the ReAct agent.\"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    \n",
    "    # Create prompt template with required variables\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"Answer the following questions as best you can.\n",
    "\n",
    "{system_prompt}\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\")\n",
    "    \n",
    "    # Get tool names for the template\n",
    "    tool_names = [tool.name for tool in tools]\n",
    "    \n",
    "    # Create the ReAct agent\n",
    "    agent = create_react_agent(\n",
    "        llm=llm,\n",
    "        tools=tools,\n",
    "        prompt=prompt\n",
    "    )\n",
    "    \n",
    "    return AgentExecutor(\n",
    "        agent=agent,\n",
    "        tools=tools,\n",
    "        verbose=debug,\n",
    "        max_iterations=3,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "\n",
    "def process_excel_query(file_name: str, user_query: str, debug: bool = True) -> dict:\n",
    "    \"\"\"Process a query about an Excel file using the ReAct agent with state tracking.\"\"\"\n",
    "    # Initialize state\n",
    "    state = create_agent_state(file_name, user_query)\n",
    "    \n",
    "    try:\n",
    "        agent = setup_agent(debug=debug)\n",
    "        input_query = f\"{user_query} Using the file: {file_name}\"\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\nüé¨ Starting Agent Execution\\nQuery: {input_query}\")\n",
    "            print_state(state, \"Initial State\")\n",
    "        \n",
    "        # Update state before execution\n",
    "        # state['attempts'] += 1\n",
    "        \n",
    "        # Pass both input and system_prompt\n",
    "        result = agent.invoke({\n",
    "            \"input\": input_query,\n",
    "            \"system_prompt\": SYSTEM_PROMPT\n",
    "        })\n",
    "        \n",
    "        # Update state after execution\n",
    "        intermediate_steps = result.get('intermediate_steps', [])\n",
    "        if intermediate_steps:\n",
    "            # Store the last action and input\n",
    "            last_step = intermediate_steps[-1]\n",
    "            try:\n",
    "                tool_info = {\n",
    "                    \"tool\": last_step[0].tool,\n",
    "                    \"input\": last_step[0].tool_input\n",
    "                }\n",
    "                state['last_query'] = json.dumps(tool_info, indent=2)\n",
    "                \n",
    "                # Try to parse the result\n",
    "                step_result = last_step[1]\n",
    "                if isinstance(step_result, str):\n",
    "                    try:\n",
    "                        state['query_result'] = json.loads(step_result)\n",
    "                    except json.JSONDecodeError:\n",
    "                        state['query_result'] = {\"result\": step_result}\n",
    "                else:\n",
    "                    state['query_result'] = {\"result\": str(step_result)}\n",
    "                \n",
    "                # Store any error messages\n",
    "                if isinstance(state['query_result'], dict):\n",
    "                    if 'error' in state['query_result']:\n",
    "                        state['error'] = state['query_result']['error']\n",
    "                    elif 'result' in state['query_result']:\n",
    "                        state['error'] = None  # Clear any previous errors\n",
    "            except Exception as e:\n",
    "                state['error'] = f\"Error processing state: {str(e)}\"\n",
    "                state['query_result'] = {\"error\": str(e)}\n",
    "        \n",
    "        if debug:\n",
    "            print(\"\\n‚úÖ Agent Execution Completed\")\n",
    "            print_state(state, \"Final State\")\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"output\": result['output'],\n",
    "            \"steps\": intermediate_steps,\n",
    "            \"state\": state\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        state['error'] = str(e)\n",
    "        if debug:\n",
    "            print(f\"\\n‚ùå Agent Execution Failed: {str(e)}\")\n",
    "            print_state(state, \"Error State\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"state\": state\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ae12c-2ed0-4a88-8f87-150b4a16814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process your query\n",
    "result = process_excel_query(\n",
    "    file_name=\"ipo_data.xlsx\",\n",
    "    user_query=\"What percentage of the proposedTickerSymbol-s whit proposedSharePrice between 0 and 15.00 have average positive performance above 10%. The performance in percentage of each ticker is available in the months 1 through 13. Dont count empty months in the calculation.\",\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "\n",
    "# Access state if needed\n",
    "final_state = result['state']\n",
    "print_state(final_state, \"Final State Review\")\n",
    "\n",
    "# Group the proposedTickerSymbol symbols by increment of 5 based on proposedSharePrice. For each group take all proposedTickerSymbols in the group and calculate the average performance of each symbol. The performance of a symbol is spread in the months 1 through 13. Pay attention to empty value months, just ignore them from the calculation and reduce the month count in this case. Once you calculate the performance of each proposedTickerSymbol then calculate the peformance of the group. I would like to see the top 3 groups by performance and the proposedTickerSymbol in them.\n",
    "# What percentage of the proposedTickerSymbol-s whit proposedSharePrice between 0 and 15.00 have average positive performance above 10%. The performance in percentage of each ticker is available in the months 1 through 13. Dont count empty months in the calculation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d6555-3f34-469c-aeab-3ffe5ec4148d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d2978-310a-4fb4-8d85-9980ce42d93c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
